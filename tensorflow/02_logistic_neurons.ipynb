{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Neurons\n",
    "```\n",
    "----------------------------------------------------------------------\n",
    "Filename : 03_logistic_neurons.ipynb\n",
    "Date     : 16th March, 2017\n",
    "Author   : Jaidev Deshpande\n",
    "Purpose  : Introduction to linear neurons with logistic output.\n",
    "Libraries: Tensorflow and its dependencies\n",
    "----------------------------------------------------------------------\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from utils import make_classification, draw_decision_boundary, sigmoid\n",
    "from sklearn.metrics import accuracy_score\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "plt.rc('figure', figsize=(8, 6))\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation of a logistic neuron:\n",
    "\n",
    "## $$ z = \\sum_{i \\in L} x_{i}w_{i} + b$$ \n",
    "\n",
    "## Predicted output:\n",
    "\n",
    "## $$ y = \\frac{1}{1 + e^{-z}} $$\n",
    "\n",
    "## Loss function: Mean Squared Error:\n",
    "## $$ E = \\frac{1}{2}\\sum_{i \\in L} (t^{i} - y^{i})^{2} $$\n",
    "## Where $L$ is the set of training cases, and $t$ is the target value\n",
    "\n",
    "# Logistic Neuron in NumPy:\n",
    "\n",
    "## Step 1: Make dummy data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "X, Y = make_classification()\n",
    "W = np.random.rand(2, 1)\n",
    "B = np.random.rand(1,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "draw_decision_boundary(W.ravel().tolist() + [B[0]], X, Y.ravel())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Get activation and prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# activation\n",
    "Z = np.dot(X, W) + B\n",
    "\n",
    "# prediction\n",
    "Y_pred = sigmoid(Z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Derive gradient for loss function\n",
    "## Gradient:  $\\nabla{E} = \\frac{\\partial{E}}{\\partial{w_{j}}}$\n",
    "\n",
    "## Trick:\n",
    "## $$\n",
    "\\begin{equation}\n",
    "            \\frac{\\partial{\\mathbf{E}}}{\\partial{\\mathbf{W}}} = \\frac{\\partial{\\mathbf{y}}}{\\partial{\\mathbf{W}}}\\frac{\\partial{\\mathbf{E}}}{\\partial{\\mathbf{y}}}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "## Second term on RHS:\n",
    "## $$\\frac{\\partial{\\mathbf{E}}}{\\partial{\\mathbf{y}}} = -(\\mathbf{t} - \\mathbf{y})$$\n",
    "\n",
    "## First term on RHS: (using same trick):\n",
    "## $$\\frac{\\partial{\\mathbf{y}}}{\\partial{\\mathbf{W}}} = \\frac{\\partial{\\mathbf{y}}}{\\partial{\\mathbf{z}}}\\frac{\\partial{\\mathbf{z}}}{\\partial{\\mathbf{W}}}$$\n",
    "\n",
    "## From first exercise, first term on RHS reduces to:\n",
    "## $$\\frac{\\partial{\\mathbf{y}}}{\\partial{\\mathbf{z}}} = \\mathbf{y}(1 - \\mathbf{y})$$\n",
    "\n",
    "## From definition of logistic activation:\n",
    "## $$\\mathbf{z} = \\mathbf{X}\\mathbf{W} + \\mathbf{b} $$\n",
    "\n",
    "## Second term in RHS:\n",
    "## $$\\frac{\\partial{\\mathbf{z}}}{\\partial{\\mathbf{W}}} = \\mathbf{X}$$\n",
    "\n",
    "## Substituting:\n",
    "## $$\\frac{\\partial{\\mathbf{y}}}{\\partial{\\mathbf{W}}} = \\mathbf{y}(1 - \\mathbf{y})\\mathbf{X}$$\n",
    "\n",
    "## Substituting back in original equation\n",
    "## $$\\frac{\\partial{\\mathbf{E}}}{\\partial{\\mathbf{W}}} = -(\\mathbf{t} - \\mathbf{y})\\mathbf{y}(1 - \\mathbf{y})\\mathbf{X}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using this gradient to train neuron with NumPy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X, weights, bias=None):\n",
    "    z = np.dot(X, weights)\n",
    "    if bias is not None:\n",
    "        z += bias\n",
    "    return sigmoid(z)\n",
    "\n",
    "def train(X, Y, weights, alpha=0.3):\n",
    "    y_hat = predict(X, weights)\n",
    "    _gw = -1 * (Y - y_hat) * y_hat * (1 - y_hat)\n",
    "    _gw = np.repeat(_gw, X.shape[1], axis=1)\n",
    "    weights -= (alpha * _gw * X).sum(0).reshape(-1, 1)\n",
    "    return weights\n",
    "\n",
    "def loss(y1, y2):\n",
    "    return (0.5 * ((y1 - y2) ** 2)).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "for i in range(10000):\n",
    "    y_hat = predict(X, W)\n",
    "    W = train(X, Y, W)\n",
    "    if i % 1000 == 0:\n",
    "        print(\"Loss: \", loss(Y, y_hat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "draw_decision_boundary(W.ravel().tolist() + [B[0]], X, Y.ravel())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise: Implement logistic neuron with Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xx = tf.constant(X, dtype='float32')\n",
    "yy = tf.constant(Y, dtype='float32')\n",
    "w = tf.random.uniform((2, 1))\n",
    "b = tf.random.uniform((1,))\n",
    "\n",
    "activation = tf.tensordot(xx, w, 1) + b\n",
    "prediction = 1 / (1 + tf.exp(-activation))\n",
    "\n",
    "loss = 0.5 * tf.reduce_sum(tf.pow(yy - prediction, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# enter code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.GradientTape() as g:\n",
    "    g.watch(w)\n",
    "    activation = tf.tensordot(xx, w, 1) + b\n",
    "    prediction = 1 / (1 + tf.exp(-activation))\n",
    "    loss = 0.5 * tf.reduce_sum(tf.pow(yy - prediction, 2))\n",
    "\n",
    "for i in range(10000):\n",
    "    with tf.GradientTape() as g:\n",
    "        g.watch(w)\n",
    "        activation = tf.tensordot(xx, w, 1) + b\n",
    "        prediction = 1 / (1 + tf.exp(-activation))\n",
    "        loss = 0.5 * tf.reduce_sum(tf.pow(yy - prediction, 2))\n",
    "    \n",
    "    grad = g.gradient(loss, w)\n",
    "    w -= 0.3 * grad\n",
    "    if i % 1000 == 0:\n",
    "        print(\"Loss: \", loss.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_decision_boundary(w.numpy().ravel().tolist() + [b.numpy()[0]], X, Y.ravel())"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
